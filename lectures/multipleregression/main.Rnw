\documentclass{beamer}
\input{BeamOptions.tex}
\begin{document}

<<setup, include=FALSE>>=
options(replace.assign=TRUE, width=40)
opts_knit$set(progress=FALSE)
library(ggplot2)
library(dplyr)
@

\title{Multiple Linear Regression}
\institute{CSU Chico, Math 314}
\date{\today}
\maketitle

\frame {\frametitle{outline}
  \tableofcontents
}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{outline}
    \tableofcontents[currentsection]
  \end{frame}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% frames %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Recap}

\begin{frame}
  \frametitle{Recap, simple linear regression}
  Simple linear regression attempts to predict the response variable $Y$ using a linear model on the explanatory variable $X$

\[ Y_i = \beta_0 + \beta_1X_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma). \]
\end{frame}

\section{Multiple Linear Regression}

\begin{frame}
  \frametitle{Multiple Regression, idea}
  Multiple regression is the extension of simple linear regression to more than one explanatory variable.  The notation is a bit more involved, but much is the same as before.
\end{frame}

\begin{frame}
  \frametitle{Multiple Regression, model}
  With multiple regression, there are indices $i = 1, \ldots, n$ for the observations and $j = 1, \ldots, k$ for the $j$th explanatory variable.  The multiple regression model is written

\[ Y_i = \beta_0 + \beta_1 X_{i1} + \ldots + \beta_k X_{ik} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma). \]
\end{frame}

\subsection{assumptions}

\begin{frame}
  \frametitle{Multiple Regression, assumptions}
The assumptions of multiple regression are almost the same as for simple linear regression.

\begin{itemize}
\item Linearity -- each variable is linearly related to the response,
\item Independent observations -- no two points are dependent on each other.
\item Constant Variability -- variation of points around least squares fit remains roughly constant, and
\item Normality -- the residuals should be nearly normal,
\pause \item Collinearity -- try to avoid collinear (think correlated) explanatory variables.
\end{itemize}
\end{frame}

\subsection{lite example}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, lite example}
We'll consider the data set \texttt{datasets::state.x77}.  Let's try to predict life expectancy, the variable named \texttt{Life Exp}.

<<cache=TRUE>>=
suppressMessages({library(ggplot2)
    library(dplyr)})
# ?state.x77
stateData <- as.data.frame(state.x77) # make data frame
names(stateData) # look at variable names
# make scatter plots
@
\end{frame}


\begin{frame}[fragile]
  \frametitle{Multiple Regression, lite example}
Let's do some prep-work to help ourselves out.
<<cache=TRUE>>=
## new variable names without spaces
st <- mutate(stateData, HSGrad = `HS Grad`,
            LifeExp = `Life Exp`,
            Density = Population*1000/Area)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, lite example}
Now fit a model with lots of variables to predict life expectancy.
<<cache=TRUE>>=
fit <- lm(LifeExp ~ Income + Illiteracy + Murder +
             HSGrad + Frost + Area + Density,
           data=st)
  # summary(fit) # Rstudio
@
\end{frame}

\subsection{interpretation}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, interpretation}
  Interpretation of the model usually invovles one slope estimate at a time. For example, we say, holding all else constant for every one day increase in the mean number of days with minimum temperature below freezing in the capital city life expectancy decreases by $0.007044$ years.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, interpretation}
  Holding all else constant for every one unit increase in the murder rate life expectancy decreases by $0.2864$ years.
\end{frame}

\subsection{adjusted $R^2$}

\begin{frame}
  \frametitle{Multiple Regression, adjusted $R^2$}
  The adjusted $R^2$ value for this model is $0.6753$.  Thus, this linear model accounts for $67.53\%$ of the variation in life expectancy.
\end{frame}

\subsection{simple model selection}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, simple model selection}
  It is generally OK to toss out variables, one at a time based on their p-value.  Drop the variable with the largest p-value and refit the model for each next dropped variable, until all the p-values are less than your pre-specified level of significance.
<<eval=FALSE>>=
## try this and watch what happens to un/adjusted R^2
@
\end{frame}

\section{Checking Assumptions}

\begin{frame}
  \frametitle{Multiple Regression, check assumptions}
  Mostly, we check the assumptions of the model just the same as before.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Turning back to our the life expectancy model.  Let's check the model assumptions.
<<cache=TRUE>>=
bestfit <- lm(LifeExp ~ Murder + HSGrad + Frost, data=st)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
  Let's extract the standardized residuals and the fitted values.
<<cache=TRUE>>=
r <- rstandard(bestfit)
yhat <- fitted(bestfit)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Residuals on fitted values -- check linearity, constant variation, and outliers.
<<>>=
linearity <- qplot(yhat, r) +
    geom_hline(aes(yintercept=0))
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Not bad -- no obvious signs of a pattern, and constant variation.
<<echo=FALSE, fig.align="center", fig.height=3, fig.width=3>>=
linearity
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
It's good, but sometimes cumbersome, to check for linearity relative to each predictor variable of your final model.
<<cache=TRUE>>=
rm <- qplot(st[,"Murder"], r)
rh <- qplot(st[,"HSGrad"], r)
rf <- qplot(st[,"Frost"], r)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Residuals on murder rate.
<<echo=FALSE, fig.align="center", fig.height=3, fig.width=3>>=
rm
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Residuals on high school graduation percentage.
<<echo=FALSE, fig.align="center", fig.height=3, fig.width=3>>=
rh
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}Residuals on mean number of frost days.
<<echo=FALSE, fig.align="center", fig.height=3, fig.width=3>>=
rf
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Histogram of residuals -- check normality.
<<>>=
normality <- qplot(r, geom="histogram", binwidth=1/3)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, check assumptions}
Seems not terrible -- not heavily skewed.
<<echo=FALSE, fig.align="center", fig.height=3, fig.width=3>>=
normality
@
\end{frame}

\section{Heavy Example}

\begin{frame}
  \frametitle{Brief Recap}
  Recall that ANOVA esimates means by a categorical variables, and linear regression estimates means by a numerical variable.  ANCOVA blends these two models, by estimating a slope on the numerical variable(s) for each level of the categorical variable(s)\footnote{Often the categorical variable is called an \textbf{indicator} variable in this context.}.
\end{frame}

\begin{frame}
  \frametitle{ANCOVA}
We can mix ANOVA and linear regression.  Some people call this analysis of covariance, ANCOVA.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}
  Consider the data set \texttt{ape::carnivora}.
<<cahce=TRUE>>=
suppressMessages(library(ape))
data(carnivora)
carnivs <- filter(carnivora,
                 Family %in% c("Canidae",
                               "Felidae",
                               "Mustelidae"))
# make box/scatter plots
@
\end{frame}



\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}
Let's try to predict body weight from birth weight, creating a new line, each with the same slope, for each family in the data set \texttt{carnivs}.
<<cache=TRUE>>=
# Recall, R needs to know Family is
# a categorical variable / factor
is.factor(carnivs[,"Family"])
carnfit <- lm(SW ~ Family + BW, data=carnivs)
# summary(carnfit) # RStudio
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}
Independent intercepts by a categorical variable.
<<cache=TRUE, echo=FALSE, fig.align="center", fig.width=3, fig.height=3>>=
beta <- coefficients(carnfit)
lines <- data.frame(int=beta[1:3], slp=beta[4])
lines <- mutate(lines, cols=factor(rownames(lines)))
qplot(BW, SW, data=carnivs, na.rm=TRUE, xlab="Birth weight (g)", ylab="Body weight (kg)")  + geom_abline(aes(intercept=int, slope=slp, colour=cols), data=lines) + theme(legend.position="none")
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}
  We could also create new slopes, with one intercept, by a categorical variable.
<<cache=TRUE>>=
carnfit2 <- lm(SW ~ Family:BW, data=carnivs)
# summary(carnfit2) # RStudio
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}

Independent slopes by a categorical variable.
<<cache=TRUE, echo=FALSE, fig.align="center", fig.width=3, fig.height=3>>=
beta <- coefficients(carnfit2)
lines <- data.frame(int=as.numeric(beta[1]), slp=as.numeric(beta[2:4]))
lines <- mutate(lines, cols=factor(rownames(lines)[2:4]))
qplot(BW, SW, data=carnivs, na.rm=TRUE, xlab="Birth weight (g)", ylab="Body weight (kg)")  + geom_abline(aes(intercept=int, slope=slp, colour=cols), data=lines) + theme(legend.position="none")
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}

  We could also create new slopes and intercepts by a categorical variable.
<<cache=TRUE>>=
carnfit3 <- lm(SW ~ Family + Family:BW, data=carnivs)
## same as
# carnfit3 = lm(SW ~ Family*BW, data=carnivs)
# summary(carnfit3) # RStudio
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Regression, heavy example}
Inpendent slopes and intercepts by a categorical.
<<cache=TRUE, echo=FALSE, fig.align="center", fig.width=3, fig.height=3>>=
beta <- coefficients(carnfit3)
lines <- data.frame(int=beta[1:3], slp=beta[4:6])
lines <- mutate(lines, cols=factor(rownames(lines)[1:3]))
qplot(BW, SW, data=carnivs, na.rm=TRUE)  + geom_abline(aes(intercept=int, slope=slp, colour=cols), data=lines) + theme(legend.position="none")
@
\end{frame}

\section{Take Away}
\begin{frame}
  \frametitle{Take Away}
  Multiple regression adds new layers of complexity to a relatively simple idea:
  \begin{itemize}
  \item<2-> fitting multiple lines across multiple explanatory variables,
  \item<3-> each line is interpretted with the other variables ``held constant,''
  \item<4-> mixing ANOVA and linear regression greatly expands our ability to model the real world
    \begin{itemize}
    \item<5-> multiple intercepts by level of categorical variables
    \item<6-> multiple slopes by level of categorical variables
    \item<7-> or both slopes and intercepts by categorical variable
    \end{itemize}
  \item<8-> Interpretation of adjusted $R^2$ remains
  \item<9-> checking model assumptions still necessary
  \end{itemize}
\end{frame}



\section{References}
\nocite{Diez:2015}
\begin{frame}[allowframebreaks]
  \frametitle{references}
  \bibliographystyle{plainnat} \bibliography{../ref}
\end{frame}

\end{document}
